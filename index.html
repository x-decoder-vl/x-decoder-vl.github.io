<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Towards a Generalized Multi-Modal Foundation Model">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Towards a Generalized Multi-Modal Foundation Model</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="icon" href="./static/images/icon.png">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  </head>
  <body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <!-- <h1 class="title is-1 publication-title">X-Decoder:</h1> -->
          <h2 class="title is-2 publication-title">Towards a Generalized Multimodal Foundation Model</h2>

          <br>
          <div class="is-size-5 publication-authors">
            Along the track of <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zou_Generalized_Decoding_for_Pixel_Image_and_Language_CVPR_2023_paper.pdf"><b>X-Decoder</b></a>, <a href="https://arxiv.org/pdf/2304.06718.pdf"><b>SEEM</b></a>, and <a href="http://arxiv.org/abs/2312.07532"><b>FIND</b></a>, our goal is to build a foundation model that extendable to a wide range of vision and vision-language tasks without any overhead!
            <!-- <span class="author-block"><b style="color:#f68946; font-weight:normal">&#x25B6 </b> University of Wisconsin-Madison; </b></span>
            <span class="author-block"><b style="color:#F2A900; font-weight:normal">&#x25B6 </b>UCLA; </span>
            <span class="author-block"><b style="color:#00A4EF; font-weight:normal">&#x25B6 </b>Microsoft Research, Redmond; </span>
            <span class="author-block"><b style="color:#008AD7; font-weight:normal">&#x25B6 </b>Microsoft Cloud & AI </span> -->
          </div>
          <br>

          <br>
        <div class="columns">
          <div class="column is-4" style="display: flex; flex-direction: column; border-right: 1px solid #ccc; padding-right: 20px;">
            <div style="margin-bottom: 20px;">
              <h2 class="subtitle" style="font-weight: bold;">
                X-Decoder <span style="font-weight: normal; font-size: 14px; color: red;">(CVPR 2023)</span>
              </h2>
              <p>
                <b>"Generalized Decoding for Pixel, Image, and Language"</b> [1] shortened as X-Decoder is a generalized decoding system that is applicable to various vision-language tasks with a unified architecture. The input and output modalities spans vision-language while the granularity spans pixel-image.
              </p>
            </div>
            <span class="link-block">
              <a href="https://github.com/microsoft/X-Decoder/tree/main" target="_blank" 
                 class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="fab fa-github"></i>
                </span>
                <span>Code &#9734; 1.1k</span>
                </a>
            </span>
            <span class="link-block">
              <a href="https://github.com/microsoft/X-Decoder/tree/xgpt"
                 class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="fab fa-hotjar"></i>
                </span>
                <span>X-GPT</span>
                </a>
            </span>
            <img id="x-decoder-teaser" style="align-self: flex-start;" width="100%" src="./images/xdecoder-t.png">
          </div>
          
          <div class="column is-4" style="display: flex; flex-direction: column; border-left: 1px solid #ccc; border-right: 1px solid #ccc; padding-left: 20px; padding-right: 20px;">
            <div style="margin-bottom: 20px;">
              <h2 class="subtitle" style="font-weight: bold;">
                SEEM <span style="font-weight: normal; font-size: 14px; color: red;">(NeurIPS 2023)</span>
              </h2>
              <p>
                <b>"Segment Everything Everywhere All-at-Once"</b> [2] denoted as SEEM in extending X-Decoder with human interaction capability, where human can refer a segment with scribble, box, point, and etc. Meanwhile, it accepts prompts span vision-language in a composite manner.
              </p>
            </div>

            <span class="link-block">
              <a href="https://github.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once" target="_blank" 
                 class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="fab fa-github"></i>
                </span>
                <span>Code &#9734; 3.6k</span>
                </a>
            </span>
            <span class="link-block">
              <a href="http://semantic-sam.xyzou.net:6090/"
                 class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="far fa-laugh-beam"></i>
                </span>
                <span>Demo</span>
                </a>
            </span>
            <img id="seem-teaser" style="align-self: flex-start;" width="100%" src="./images/seem-t.png">
          </div>
          
          <div class="column is-4" style="display: flex; flex-direction: column; border-left: 1px solid #ccc; padding-left: 20px;">
            <div style="margin-bottom: 20px;">
              <h2 class="subtitle" style="font-weight: bold;">
                FIND <span style="font-weight: normal; font-size: 14px; color: red;">(New!)</span>
              </h2>
              <p>
                "Interfacing Foundation Models' Embeddings" denoted as FIND introduces a generalized interface for foundation models. It aligns an embedding space with interleaved reasoning capability for pretrained-and-fixed foundation models (e.g. X-Decoder, UniCL, SAM, LLaMA).
              </p>
            </div>
            <span>
              <a href="https://github.com/UX-Decoder/FIND" target="_blank" 
                 class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="fab fa-github"></i>
                </span>
                <span>Code</span>
                </a>
            </span>
            <span class="link-block">
              <a href=""
                 class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="far fa-laugh-beam"></i>
                </span>
                <span>Demo</span>
                </a>
            </span>
            <img id="find-teaser" style="align-self: flex-start;" width="100%" src="./images/find-t.png">
          </div>
        </div>


          <!-- <div class="is-size-5">
            <span class="author-block">
                <a href="https://maureenzou.github.io/" style="color:#f68946;font-weight:normal;">Xueyan Zou<sup>*</sup>
                </a>,                
            </span>
            <span class="author-block">
              <a href="https://zdou0830.github.io/" style="color:#F2A900;font-weight:normal;">Zi-Yi Dou<sup>*</sup></a>,</span>
            <span class="author-block">
              <a href="https://jwyang.github.io/" style="color:#00A4EF;font-weight:normal;">Jianwei Yang<sup>*&#x2691;</sup></a>,
            </span>
            <span class="author-block">
              <a href="https://zhegan27.github.io/" style="color:#008AD7;font-weight:normal;">Zhe Gan</a>,
            </span>
            <span class="author-block">
              <a href="https://www.microsoft.com/en-us/research/people/linjli/" style="color:#008AD7;font-weight:normal;">Linjie Li</a>,
            </span>
            <span class="author-block">
              <a href="https://chunyuan.li/" style="color:#00A4EF;font-weight:normal;">Chunyuan Li</a>,
            </span>  
            <span class="author-block">
              <a href="https://sites.google.com/site/xiyangdai/" style="color:#008AD7;font-weight:normal;">Xiyang Dai</a>,
            </span>   
            <span class="author-block">
              <a href="https://harkiratbehl.github.io/" style="color:#00A4EF;font-weight:normal;">Harkirat Behl</a>,
            </span>                                  
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=vJWEw_8AAAAJ&hl=en" style="color:#008AD7;font-weight:normal;">Jianfeng Wang</a>,
            </span>
            <span class="author-block">
              <a href="https://www.microsoft.com/en-us/research/people/luyuan/" style="color:#008AD7;font-weight:normal;">Lu Yuan</a>,
            </span>       
            <span class="author-block">
              <a href="https://vnpeng.net/" style="color:#F2A900;font-weight:normal;">Nanyun Peng</a>,
            </span>       
            <span class="author-block">
              <a href="https://www.microsoft.com/en-us/research/people/lijuanw/" style="color:#008AD7;font-weight:normal;">Lijuan Wang</a>,
            </span>    
            <span class="author-block">
              <a href="https://pages.cs.wisc.edu/~yongjaelee/" style="color:#f68946;font-weight:normal;">Yong Jae Lee<sup>&#x2628;</sup></a>,
            </span>                                                
            <span class="author-block">
              <a href="https://www.microsoft.com/en-us/research/people/jfgao/" style="color:#00A4EF;font-weight:normal;">Jianfeng Gao<sup>&#x2628;</sup></a>
            </span>
          </div>

          <br>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><b style="color:#f68946; font-weight:normal">&#x25B6 </b> University of Wisconsin-Madison; </b></span>
            <span class="author-block"><b style="color:#F2A900; font-weight:normal">&#x25B6 </b>UCLA; </span>
            <span class="author-block"><b style="color:#00A4EF; font-weight:normal">&#x25B6 </b>Microsoft Research, Redmond; </span>
            <span class="author-block"><b style="color:#008AD7; font-weight:normal">&#x25B6 </b>Microsoft Cloud & AI </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>*</sup>Equal Technical Contribution, </span>
            <span class="author-block"><sup>&#x2628;</sup>Equal Advisory Contribution, </span>
            <span class="author-block"><sup>&#x2691;</sup>Project Lead </span>
          </div>

          <br>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><b style="color:#e08ba0; font-weight:normal"> <b>In CVPR2023</b> </b></span>
          </div> -->

        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <b><span style="font-size: 20px; color:#ea7e56;">&#x25B6; FIND</span></b>
      <img id="teaser" width="120%" src="images/find-gif.gif">
      <h2 class="subtitle has-text-centered">
        <p style="font-family:Times New Roman"><b>FIND interfacing foundation models for an interleaved embedding space. </b></p>
      </h2>
    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <b><span style="font-size: 20px; color:#ea7e56;">&#x25B6; SEEM</span></b>
      <img id="teaser" width="120%" src="images/seem_gif.gif">
      <h2 class="subtitle has-text-centered">
        <p style="font-family:Times New Roman"><b>SEEM supports a wide range of prompt types including text, scribble, ref-image, and etc. </b></p>
      </h2>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <b><span style="font-size: 20px; color:#ea7e56;">&#x25B6; X-Deocder</span></b>
      <img id="teaser" width="120%" src="images/tittle_fig.gif">
      <h2 class="subtitle has-text-centered">
        <p style="font-family:Times New Roman"><b>X-Decoder is a single model trained to support a wide range of vision and vision-language tasks.</b></p>
      </h2>
    </div>
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
    @article{zou2023xdecoder,
      author      = {Xueyan Zou*, Zi-Yi Dou*, Jianwei Yang*, Zhe Gan, Linjie Li, Chunyuan Li, Xiyang Dai, Harkirat Behl, Jianfeng Wang, Lu Yuan, Nanyun Peng, Lijuan Wang, Yong Jae Lee*, Jianfeng Gao*},
      title       = {Generalized Decoding for Pixel, Image and Language},
      publisher   = {CVPR},
      year        = {2023},
    }
    @article{zou2023seem,
      author      = {Xueyan Zou*, Jianwei Yang*, Hao Zhang*, Feng Li*, Linjie Li, Jianfeng Wang, Lijuan Wang, Jianfeng Gao*, Yong Jae Lee*},
      title       = {Segment everything everywhere all at once},
      publisher   = {NeurIPS},
      year        = {2023},
    }
    @article{zou2023find,
      author      = {Xueyan Zou, Linjie Li, Jianfeng Wang, Jianwei Yang, Mingyu Ding, Zhengyuan Yang, Feng Li, Hao Zhang, Shilong Liu, Arul Aravinthan, Yong Jae Lee*, Lijuan Wang*},
      title       = {Generalized Decoding for Pixel, Image and Language},
      publisher   = {arXiv},
      year        = {2023},
    }
  </code></pre>
  </div>
</section>

<section class="section" id="Acknowledgement">
  <div class="container is-max-desktop content">
    <h2 class="title">Acknowledgement</h2>
    <p>
      This website is adapted from <a
      href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>, licensed under a <a rel="license"
                                          href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
      Commons Attribution-ShareAlike 4.0 International License</a>.
    </p>
  </div>
</section>


</body>
</html>
